{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi there, welcome to my project.\n\nI wanted to get some basic practice creating models, something where I could train either regression or categorical models on the data depending on feature selection. I haven't played with this one in a while now, but if I come back to it, adding sentiment analysis would be the next step. See if I can narrow down some feature or threshold that I can pick something useful out of, possibly by narrowing it based on common stock behavior, going with all tech stocks for example. The idea was to see if the model could predict either via regression giving us a predicted Close, or categorically based on threshold gains/losses. Basically trying to predict tomorrow's stock prices and go find the option based on it. If I found anything useful, the next step would be to analyze which options would be the most profitable, followed by testing with a virtual account. But in any case, some old learning code...","metadata":{}},{"cell_type":"markdown","source":"<hr>\n<h2>Step 1 - Define our Goal</h2>\nWhat do we want to do? Get rich of course. But how?<br>\n<br>\nWe're going to try to predict when either of two scenarios occur for any of a given selection of stocks:<br>\na) tomorrow's Close price will be significantly higher than today's Close value<br>\nb) tomorrow's Close price will be significantly lower than today's Close value<br>\n<br>\nAnd then we're going to buy options, either put or call, based on those predictions first thing in the morning right after the market opens and selling them at the end of the day just before the market closes. By using options, being able to predict changes in price of even a single percentage point are very very valuable.<br>\n<br>\nAt the end of all this, we want to be able to have a script that runs after every trading day. What we want from that script are our top picks for large changes in price in either direction for the following day. We want to then log in first thing in the morning and buy the predicted gainers, to be sold the following morning when the exchange opens before buying for the day. All gambles are on 1 day jumps or drops in price using put or call options accordingly.","metadata":{}},{"cell_type":"markdown","source":"<hr>\n<h2>Step 2 - Get our data</h2>\nWe'll be using yfinance to get our trading history, so first we need to install that. Then the usual imports.<br>","metadata":{}},{"cell_type":"code","source":"!pip install -q yfinance","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:33:48.526845Z","iopub.execute_input":"2025-01-29T02:33:48.527064Z","iopub.status.idle":"2025-01-29T02:34:17.464367Z","shell.execute_reply.started":"2025-01-29T02:33:48.527043Z","shell.execute_reply":"2025-01-29T02:34:17.463197Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# importing our libraries\nimport numpy\nimport pandas\nimport matplotlib.pyplot as pyplot\nimport yfinance\nimport tensorflow.keras as keras\nimport seaborn\nimport os\nimport sklearn\n\n# importing some methods and classes just so we don't have to type the package names every time\nfrom pandas import DataFrame, Series\nfrom tensorflow.data import Dataset\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Bidirectional, LSTM\nfrom tensorflow.keras.optimizers import SGD, RMSprop, Adam, AdamW, Adadelta, Adagrad, Adamax, Adafactor, Nadam, Ftrl\nfrom tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\nfrom tensorflow.keras.utils import Sequence\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:17.466357Z","iopub.execute_input":"2025-01-29T02:34:17.466660Z","iopub.status.idle":"2025-01-29T02:34:28.660061Z","shell.execute_reply.started":"2025-01-29T02:34:17.466636Z","shell.execute_reply":"2025-01-29T02:34:28.659347Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"I've selected companies with market valuations above 10 billion dollars traded on American stock exchanges. From that I've removed the first 4 years of data, tossing out the startup behavior as I won't be predicting behavior on new companies, I'm looking for macro patterns that are less susceptible to variations from general noise. I'll be keeping the last year of data as our validation data, the most relevant to today's behavior. We're also going to have to provide a window of stock history, which I will remove before the validation set so no portion of the validation set has been seen by the model before. Any stocks that didn't have enough data for that got tossed. Below is what we have left.<br>\n<br>\n<hr>\n<br>\n<h3>TODO: find which stocks correlate behavior, would a subset of similarly behaved stocks improve our predictions? Find out.</h3>\n<h3>TODO: make this automaticaly determined.</h3>\n<h3>TODO: narrow this to a single exchange perhaps? NASDAQ or TSE for example.</h3>\n<br>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"Downloading the stock histories for the above companies.","metadata":{}},{"cell_type":"code","source":"def downloadHistories(tickerNames):\n    histories = {}\n    for name in tickerNames:\n        ticker = yfinance.Ticker(name)\n        histories[name] = ticker.history(period='max')\n    return histories\n\ndef downloadHistoriesByTicker(tickerNames, start=None, end=None, period=None):\n    historiesByTicker = {}\n    for name in tickerNames:\n        ticker = yfinance.Ticker(name)\n        if period is None:\n            historiesByTicker[name] = ticker.history(start=start, end=end)\n        else:\n            historiesByTicker[name] = ticker.history(period=period)\n    return historiesByTicker\n\ndef getInitialData(tickerNames):\n    trainingStockHistoriesByTicker = downloadHistoriesByTicker(tickerNames, start='2010-01-01', end='2020-01-01')\n    testStockHistoriesByTicker = downloadHistoriesByTicker(tickerNames, period='2y')\n    return trainingStockHistoriesByTicker, testStockHistoriesByTicker","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.661003Z","iopub.execute_input":"2025-01-29T02:34:28.661447Z","iopub.status.idle":"2025-01-29T02:34:28.667474Z","shell.execute_reply.started":"2025-01-29T02:34:28.661423Z","shell.execute_reply":"2025-01-29T02:34:28.666696Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"<hr>\n<h2>Step 3 - Feature selection</h2>\nLooks good. Now to add in additional features to our dataframes and drop anything we don't want.<br>\n<br>\nFor new features, we'll be adding:<br>\nGain - the percentage gain or loss of a stock from the Open to Close price (unbounded positive or negative float). We're going to cap this at a 10 percent gain or loss in a day for scaling. This will be our feature set, and for our regression models, our labels.<br>\nThe following labels will be our labels for our binary classifier models.<br>\nGain05 - 1.0 if Gain above 0.005, 0.0 otherwise.<br>\nGain10 - 1.0 if Gain above 0.01, 0.0 otherwise.<br>\nGain15 - 1.0 if Gain above 0.015, 0.0 otherwise.<br>\nLoss05 - 1.0 if Gain below -0.005, 0.0 otherwise.<br>\nLoss10 - 1.0 if Gain below -0.01, 0.0 otherwise.<br>\nLoss15 - 1.0 if Gain below -0.015, 0.0 otherwise.<br>\nFor dropped features, we're getting rid of everything else, we're going to try treating it as a univariate time series problem first, see if that works since it's fairly simple and hopefully faster to train.","metadata":{}},{"cell_type":"code","source":"def makeGain05Label(value):\n    if value >= 0.005:\n        return 1.0\n    return 0.0\n\ndef makeGain10Label(value):\n    if value >= 0.010:\n        return 1.0\n    return 0.0\n\ndef makeGain15Label(value):\n    if value >= 0.015:\n        return 1.0\n    return 0.0\n\ndef makeLoss05Label(value):\n    if value <= -0.005:\n        return 1.0\n    return 0.0\n\ndef makeLoss10Label(value):\n    if value <= -0.010:\n        return 1.0\n    return 0.0\n\ndef makeLoss15Label(value):\n    if value <= -0.015:\n        return 1.0\n    return 0.0\n\ndef capGain10(value):\n    if value > 0.1:\n        return 0.1\n    if value < -0.1:\n        return -0.1\n    return value\n\ndef getFeaturesByTicker(tickerNames, stockHistories):\n    featuresByTicker = {}\n    \n    for name in tickerNames:\n        history = stockHistories[name].copy()\n        history['Gain'] = (history['Close'] - history['Open']) / history['Close']\n        history['Gain'] = history['Gain'].apply(capGain10)\n        history['Gain05'] = history['Gain'].apply(makeGain05Label)\n        history['Gain10'] = history['Gain'].apply(makeGain10Label)\n        history['Gain15'] = history['Gain'].apply(makeGain15Label)\n        history['Loss05'] = history['Gain'].apply(makeLoss05Label)\n        history['Loss10'] = history['Gain'].apply(makeLoss10Label)\n        history['Loss15'] = history['Gain'].apply(makeLoss15Label)\n        history = history.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1)\n        # history = history.drop(['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits'], axis=1)\n        featuresByTicker[name] = history\n        \n    return featuresByTicker","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:36:00.582373Z","iopub.execute_input":"2025-01-29T02:36:00.583246Z","iopub.status.idle":"2025-01-29T02:36:00.591346Z","shell.execute_reply.started":"2025-01-29T02:36:00.583211Z","shell.execute_reply":"2025-01-29T02:36:00.590497Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"Looks good. Now to make something useful out of it.<br>\n<br>\n<hr>\n<br>\n<h2>Step 4 - Process the data</h2>\nSo now that we have our raw data and changed it into our feature set, time to process this into something we can use.<br>\n<br>\nFor scaling, I'm going to use a threshold of a 10 percent gain or loss to capture our trading data. If a value change exceeds this threshold, I will cap it at 10 percent for our training. This shouldn't have a major impact on many values and allows us to use all the data. After that, we should have all our values lying within a range of -0.1 <= x <= 0.1, so dividing by our threshold gives us -1.0 <= x <= 1.0, so we add one to that to shift it, giving us 0.0 <= x <= 2.0, and then divide by 2 to give us our desired scaling of 0.0 <= x <= 1.0. Obviously we'll have to reverse that at the other end to get our predictions.<br>\n<br>\nAfter that, just break it into windows.","metadata":{}},{"cell_type":"code","source":"def scaleDataByTicker(featuresByTicker):\n    results = {}\n    \n    for name in tickerNames:\n        features = featuresByTicker[name].copy()\n        features['Gain'] = ((features['Gain'] / 0.1) + 1.0) / 2.0\n        results[name] = features\n        \n    return results\n\ndef mergeDataFrameDict(featuresByTicker):\n    mergedData = None\n    for name in tickerNames:\n        data = featuresByTicker[name]\n        if mergedData is None:\n            mergedData = data\n        else:\n            mergedData = mergedData.append(data)\n    return mergedData","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.687851Z","iopub.execute_input":"2025-01-29T02:34:28.688131Z","iopub.status.idle":"2025-01-29T02:34:28.699071Z","shell.execute_reply.started":"2025-01-29T02:34:28.688111Z","shell.execute_reply":"2025-01-29T02:34:28.698273Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def getWindows(features, windowSize):\n    X = []\n    y = []\n    yGain05 = []\n    yGain10 = []\n    yGain15 = []\n    yLoss05 = []\n    yLoss10 = []\n    yLoss15 = []\n    \n    numWindows = len(features) - windowSize\n    for index in range(0, numWindows):\n        featureWindow = features[index:index + windowSize + 1]\n        X.append(featureWindow['Gain'][:-1])\n        y.append(featureWindow['Gain'][-1])\n        yGain05.append(featureWindow['Gain05'][-1])\n        yGain10.append(featureWindow['Gain10'][-1])\n        yGain15.append(featureWindow['Gain15'][-1])\n        yLoss05.append(featureWindow['Loss05'][-1])\n        yLoss10.append(featureWindow['Loss10'][-1])\n        yLoss15.append(featureWindow['Loss15'][-1])\n    \n    X = numpy.array(X)\n    X = X.reshape(X.shape[0], X.shape[1], 1)\n    y = numpy.array(y)\n    yGain05 = numpy.array(yGain05)\n    yGain10 = numpy.array(yGain10)\n    yGain15 = numpy.array(yGain15)\n    yLoss05 = numpy.array(yLoss05)\n    yLoss10 = numpy.array(yLoss10)\n    yLoss15 = numpy.array(yLoss15)\n    \n    return (X, y, yGain05, yGain10, yGain15, yLoss05, yLoss10, yLoss15)\n\ndef getWindowsByTicker(featuresByTicker, windowSize):\n    windowsByTicker = {}\n    for name in tickerNames:\n        data = featuresByTicker[name]\n        windowsByTicker[name] = getWindows(data, windowSize)\n    return windowsByTicker","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.699887Z","iopub.execute_input":"2025-01-29T02:34:28.700079Z","iopub.status.idle":"2025-01-29T02:34:28.713217Z","shell.execute_reply.started":"2025-01-29T02:34:28.700062Z","shell.execute_reply":"2025-01-29T02:34:28.712550Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def getScaledDataByTicker(tickerNames):\n    trainingStockHistoriesByTicker, testStockHistoriesByTicker = getInitialData(tickerNames)\n    \n    trainingFeaturesByTicker = getFeaturesByTicker(tickerNames, trainingStockHistoriesByTicker)\n    testFeaturesByTicker = getFeaturesByTicker(tickerNames, testStockHistoriesByTicker)\n    \n    scaledTrainingDataByTicker = scaleDataByTicker(trainingFeaturesByTicker)\n    scaledTestDataByTicker = scaleDataByTicker(testFeaturesByTicker)\n    \n    return scaledTrainingDataByTicker, scaledTestDataByTicker","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.714383Z","iopub.execute_input":"2025-01-29T02:34:28.714757Z","iopub.status.idle":"2025-01-29T02:34:28.724375Z","shell.execute_reply.started":"2025-01-29T02:34:28.714728Z","shell.execute_reply":"2025-01-29T02:34:28.723782Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def getRegressionData(tickerNames, featureWindowsByTicker):\n    X = None\n    y = None\n    \n    for name in tickerNames:\n        featureWindow = featureWindowsByTicker[name]\n        X = featureWindow[0] if X is None else numpy.vstack((X, featureWindow[0]))\n        y = featureWindow[1] if y is None else numpy.concatenate((y, featureWindow[1]), axis=0)\n    \n    return X, y","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.725312Z","iopub.execute_input":"2025-01-29T02:34:28.725571Z","iopub.status.idle":"2025-01-29T02:34:28.738017Z","shell.execute_reply.started":"2025-01-29T02:34:28.725550Z","shell.execute_reply":"2025-01-29T02:34:28.737323Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Define a custom data generator to provide a random subset of a larger dataset\nclass RandomSubsetDataGenerator(Sequence):\n    def __init__(self, X, y, batch_size):\n        self.X = X\n        self.y = y\n        self.batch_size = batch_size\n        self.indexes = numpy.arange(len(self.X))\n\n    def __len__(self):\n        return int(numpy.ceil(len(self.X) / self.batch_size))\n\n    def __getitem__(self, index):\n#         start = index * self.batch_size\n#         end = (index + 1) * self.batch_size\n        batch_indexes = numpy.random.choice(self.indexes, size=self.batch_size, replace=False)\n        batch_X = self.X[batch_indexes]\n        batch_y = self.y[batch_indexes]\n        return batch_X, batch_y","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.738870Z","iopub.execute_input":"2025-01-29T02:34:28.739092Z","iopub.status.idle":"2025-01-29T02:34:28.747986Z","shell.execute_reply.started":"2025-01-29T02:34:28.739072Z","shell.execute_reply":"2025-01-29T02:34:28.747308Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def getBinaryData(tickerNames, featureWindowsByTicker, column):\n    X_true = []\n    y_true = []\n    X_false = []\n    y_false = []\n    \n    for name in tickerNames:\n        featureWindow = featureWindowsByTicker[name]\n        \n        for X, y in zip(numpy.nditer(featureWindow[0]), numpy.nditer(featureWindow[column])):\n            if y == 1.0:\n                X_true.append(X)\n                y_true.append(y)\n            else:\n                X_false.append(X)\n                y_false.append(y)\n    \n    return X_true, y_true, X_false, y_false","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.748788Z","iopub.execute_input":"2025-01-29T02:34:28.749045Z","iopub.status.idle":"2025-01-29T02:34:28.758615Z","shell.execute_reply.started":"2025-01-29T02:34:28.749018Z","shell.execute_reply":"2025-01-29T02:34:28.757857Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Define a custom data generator to provide a random subset of a larger dataset, but keep the number of true and the number of false equal\nclass RandomPairedSubsetDataGenerator(Sequence):\n    def __init__(self, X_true, y_true, X_false, y_false, batch_size):\n        self.X_true = X_true\n        self.y_true = y_true\n        self.indexes_true = numpy.arange(len(self.X_true))\n        self.X_false = X_false\n        self.y_false = y_false\n        self.indexes_false = numpy.arange(len(self.X_false))\n        self.batch_size = batch_size\n    \n    def __len__(self):\n        length = len(self.X_true) + len(self.X_false)\n        return int(numpy.ceil(length / self.batch_size))\n    \n    def __getitem__(self, index):\n        batch_indexes_true = numpy.random.choice(self.indexes_true, size=self.batch_size/2, replace=False)\n        batch_indexes_false = numpy.random.choice(self.indexes_false, size=self.batch_size/2, replace=False)\n        batch_X_true = self.X_true[batch_indexes_true]\n        batch_y_true = self.y_true[batch_indexes_true]\n        batch_X_false = self.X_true[batch_indexes_false]\n        batch_y_false = self.X_true[batch_indexes_false]\n        \n        batch_X = numpy.hstack((batch_X_true, batch_X_false))\n        batch_y = numpy.hstack((batch_y_true, batch_y_false))\n        batch_X, batch_y = sklearn.utils.shuffle(batch_X, batch_y)\n        \n        return batch_X, batch_y","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.759560Z","iopub.execute_input":"2025-01-29T02:34:28.759799Z","iopub.status.idle":"2025-01-29T02:34:28.772910Z","shell.execute_reply.started":"2025-01-29T02:34:28.759780Z","shell.execute_reply":"2025-01-29T02:34:28.772239Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Okay, we now have our data, we've gone through and selected the parts that we want, and everything is scaled and ready to be used. This should be plenty of data for now, we'll be training on a small subset initially or this will take forever. Onwards to the model building.<br>\n<br>\n<hr>\n<h2>Step 5 - Build a model</h2>\nTime to start building our models. Some to try would be simple neural networks, RandomForestModel, XGBoost, vanilla or stacked LSTMs, Bidirectional LSTMs, GRUs, Convolutional LSTMs, and Simple RNNs. We'll try both MAE and MSE for loss functions. For optimizers we'll try all of them.<br>\n<br>\nSo for our LSTM with five alternating layers of LSTM with 16 nodes and a 0.2 dropout, our early results show MAE as our loss function for the regression model along with SGD showed the best results. Early results for the binary classification models both looked extremely good, but need to dig into those results a bit more next.","metadata":{}},{"cell_type":"code","source":"def basicLSTM(size, batchSize, daysOfHistory, columnCount):\n    model = Sequential([\n        LSTM(size, return_sequences=True, input_shape=(daysOfHistory, columnCount), batch_input_shape=(batchSize, daysOfHistory, columnCount), activation='relu'),\n        LSTM(size, return_sequences=True, activation='relu'),\n        LSTM(size, return_sequences=True, activation='relu'),\n        LSTM(size, return_sequences=True, activation='relu'),\n        LSTM(size, return_sequences=False, activation='relu'),\n        Dense(1)\n    ])\n    return model\n\ndef basicLSTM_binary(size, batchSize, daysOfHistory, columnCount):\n    model = Sequential([\n        LSTM(size, return_sequences=True, input_shape=(daysOfHistory, columnCount), batch_input_shape=(batchSize, daysOfHistory, columnCount), activation='relu'),\n        LSTM(size, return_sequences=True, activation='relu'),\n        LSTM(size, return_sequences=True, activation='relu'),\n        LSTM(size, return_sequences=True, activation='relu'),\n        LSTM(size, return_sequences=False, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    return model","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.773937Z","iopub.execute_input":"2025-01-29T02:34:28.774797Z","iopub.status.idle":"2025-01-29T02:34:28.782744Z","shell.execute_reply.started":"2025-01-29T02:34:28.774768Z","shell.execute_reply":"2025-01-29T02:34:28.781973Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Okay, looks good, but this is far too much data to train on right now, let's take a sampling. Once we settle on our model of choice we can train it longer. We'll need to create a DataGenerator to do the trick.","metadata":{}},{"cell_type":"code","source":"def loadOrTrainModel(modelName, model, X, y, X_test, y_test):\n    modelPath = '/kaggle/working/' + str(modelName) + '.csv'\n    if os.path.isfile(modelPath):\n        print(modelName, 'already trained, loading history')\n        modelHistoryDataFrame = pandas.read_csv(modelPath)\n    else:\n        train_data_generator = RandomSubsetDataGenerator(X, y, batchSize * 50)\n        test_data_generator = RandomSubsetDataGenerator(X_test, y_test, batchSize * 10)\n        earlyStoppingCallback = keras.callbacks.EarlyStopping(patience=5, min_delta=0.0005, restore_best_weights=True,)\n        \n        print('training', modelName)\n        modelHistory = model.fit(train_data_generator,\n                                 validation_data=test_data_generator,\n                                 epochs=epochs,\n                                 steps_per_epoch=len(train_data_generator),\n                                 validation_steps=len(test_data_generator),\n                                 verbose=1,\n                                 batch_size=batchSize,\n                                 callbacks=[earlyStoppingCallback])\n        modelHistoryDataFrame = DataFrame(modelHistory.history)\n        modelHistoryDataFrame.to_csv(modelPath)\n    \n    return modelHistoryDataFrame\n\ndef trainRegressionModel(modelName, Model, X, y, X_test, y_test):\n    modelPath = '/kaggle/working/' + str(modelName) + '.csv'\n    train_data_generator = RandomSubsetDataGenerator(X, y, batchSize * 50)\n    test_data_generator = RandomSubsetDataGenerator(X_test, y_test, batchSize * 10)\n    earlyStoppingCallback = keras.callbacks.EarlyStopping(patience=5, min_delta=0.0001, restore_best_weights=True,)\n    print('\\n******************************\\n')\n    print('training', modelName)\n    print('\\n******************************\\n')\n    modelHistory = model.fit(train_data_generator,\n                             validation_data=test_data_generator,\n                             epochs=epochs,\n                             steps_per_epoch=len(train_data_generator),\n                             validation_steps=len(test_data_generator),\n                             verbose=1,\n                             batch_size=batchSize,\n                             callbacks=[earlyStoppingCallback])\n    modelHistoryDataFrame = DataFrame(modelHistory.history)\n    modelHistoryDataFrame.to_csv(modelPath)\n    \n    return modelHistoryDataFrame\n\ndef loadModelHistory(modelName):\n    modelPath = '/kaggle/working/' + str(modelName) + '.csv'\n    if os.path.isfile(modelPath):\n        print(modelName, 'already trained, loading history')\n        modelHistoryDataFrame = pandas.read_csv(modelPath)\n        return modelHistoryDataFrame\n    \n    return None","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.783683Z","iopub.execute_input":"2025-01-29T02:34:28.783881Z","iopub.status.idle":"2025-01-29T02:34:28.797721Z","shell.execute_reply.started":"2025-01-29T02:34:28.783864Z","shell.execute_reply":"2025-01-29T02:34:28.796934Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"<hr>\n<h2>Step 6 - Look at our results</h2>\nOkay, from those first LSTM results, the binary classification models appear to be far superior to the regression models. But to make sure, let's go see what we have for actual results and predictions. If these results pan out, will try multiple layers of binary classifiers with different thresholds, and taking the stocks that get predicted by multiple threshold models for the highest gains and additional accuracy hopefully.<br>\n<br>\nFirst off, let's go see what we have for labels and predictions for each of the stocks. Let's see how much of a change we have vs. what we're predicting with each model.\nWhat constitutes good results though?  For our regression model we'll be using mean absolute error as our loss function. A successful result is one in which we predict a useful percentage of price changes larger than the mean absolute error. For the binary classifiers we're looking for predicting enough of them to be useful and being correct enough times to make it profitable.<br>\n<br>\nAs for what to measure it against, we need to keep in mind our same criteria for selecting training windows, namely the entire 250 day trading history needs to fall within a ten percent change in either direction and be one of the companies in the tickerNames list. Our trading strategy to test with is to purchase either call or put options based on whether it is a predicted gain or loss. The options chain we'll be using is the second one, so the one that ends the following week, but we should test that to see which makes the most money. Finally, we want to look at each stock individually. Do certain stocks behave more predictably than others?","metadata":{}},{"cell_type":"code","source":"# def getBinaryComparison(predictions, actuals):\n#     results = {}\n#     successfulPredictions = 0\n#     actualJumps = 0\n#     missedJumps = 0\n#     badPredictions = 0\n    \n#     for prediction, actual in zip(numpy.nditer(predictions), numpy.nditer(actuals)):\n#         if actual == 1.0:\n#             actualJumps = actualJumps + 1\n#             if prediction >= 0.5:\n#                 successfulPredictions = successfulPredictions + 1\n#             else:\n#                 missedJumps = missedJumps + 1\n#         else:\n#             if prediction >= 0.5:\n#                 badPredictions = badPredictions + 1\n    \n#     results['total predictions'] = len(predictions)\n#     results['successfulPredictions'] = successfulPredictions\n#     results['actualJumps'] = actualJumps\n#     results['missedJumps'] = missedJumps\n#     results['badPredictions'] = badPredictions\n    \n#     return results","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.801037Z","iopub.execute_input":"2025-01-29T02:34:28.801277Z","iopub.status.idle":"2025-01-29T02:34:28.811287Z","shell.execute_reply.started":"2025-01-29T02:34:28.801257Z","shell.execute_reply":"2025-01-29T02:34:28.810541Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# actuals = ((y_test.squeeze() * 2) - 1) * 10\n# print('actuals[0:10]:', actuals[0:10], '\\n')\n# binaryActuals = y_gain10_test.squeeze()\n# print('binaryActuals[0:10]:', binaryActuals[0:10], '\\n')\n\n# binary8Predictions = model8binary.predict(X_test).squeeze()\n# print('binary8Predictions[:10]:', binary8Predictions[0:10])\n# binary8Comparison = getBinaryComparison(binary8Predictions, binaryActuals)\n# print('binary8Comparison', binary8Comparison, '\\n')\n\n# binary16Predictions = model16binary.predict(X_test).squeeze()\n# print('binary16Predictions[:10]:', binary16Predictions[0:10])\n# binary16Comparison = getBinaryComparison(binary16Predictions, binaryActuals)\n# print('binary16Comparison', binary16Comparison, '\\n')\n\n# averageChange = numpy.mean(numpy.abs(actuals))\n\n# model8Predictions = ((model8.predict(X_test).squeeze() * 2) - 1) * 10\n# print('model8Predictions[:10]:', model8Predictions[0:10], '\\n')\n# print('total predictions:', len(model8Predictions))\n# model8mae = numpy.mean(numpy.abs(actuals - model8Predictions))\n# print('model8 mae:', model8mae, '\\n')\n# print('average change:', averageChange)\n\n# model16Predictions = ((model16.predict(X_test).squeeze() * 2) - 1) * 10\n# print('model16Predictions[:10]:', model16Predictions[0:10], '\\n')\n# print('total predictions:', len(model16Predictions))\n# model16mae = numpy.mean(numpy.abs(actuals - model16Predictions))\n# print('model16 mae:', model16mae, '\\n')\n# print('average change:', averageChange)","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.812127Z","iopub.execute_input":"2025-01-29T02:34:28.812330Z","iopub.status.idle":"2025-01-29T02:34:28.826307Z","shell.execute_reply.started":"2025-01-29T02:34:28.812312Z","shell.execute_reply":"2025-01-29T02:34:28.825702Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"And ... no useful results. But we at least now have a way to test it. Time to try some other models. When we have better results we'll take the time to implement the trading strategy and verify actual profit levels. So for our regression models, we need our MAE to be significantly lower than our average change. For our classifier models, we need a useful percentage of big changes to be predicted and we need enough big predictions to be accurate to turn a profit.","metadata":{}},{"cell_type":"markdown","source":"<hr>\nAnd to put it all in one place ...","metadata":{}},{"cell_type":"code","source":"tickerNames = ['msft', 'aapl', 'goog', 'amzn', 'nvda', 'meta', 'tsla', 'tsm', 'avgo', 'orcl', 'asml', 'amd', 'adbe', 'crm', 'nflx', 'csco', 'intc',\n               'sap', 'intu', 'qcom', 'txn', 'ibm', 'now', 'amat', 'bkng', 'sony', 'lrcx', 'panw', 'shop', 'adp', 'adi', 'mu', 'meli', 'fi', 'klac',\n               'anet', 'cdns', 'snps', 'wday', 'eqix', 'pypl', 'team', 'mrvl', 'dell', 'rop', 'nxpi', 'adsk', 'mchp', 'ftnt', 'tel', 'stm', 'sq',\n               'iqv', 'ea', 'fis', 'csgp', 'gpn', 'veev', 'ttd', 'on', 'fico', 'mpwr', 'anss', 'hubs', 'hpq', 'mdb', 'ttwo', 'snap', 'keys', 'splk',\n               'grmn', 'smci', 'ebay', 'ptc', 'se', 'expe', 'algn', 'asx', 'hpe', 'umc', 'eric', 'nok', 'chkp', 'akam', 'ntap', 'tyl', 'stx', 'entg',\n               'fds', 'epam', 'swks', 'gddy', 'ldos', 'ssnc', 'gen', 'logi', 'enph', 'manh', 'okta', 'ntnx', 'nice', 'twlo', 'pstg', 'azpn', 'zbra',\n               'trmb', 'jkhy', 'roku', 'jnpr', 'payc', 'otex', 'ffiv', 'dox', 'qrvo']","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:28.827388Z","iopub.execute_input":"2025-01-29T02:34:28.827691Z","iopub.status.idle":"2025-01-29T02:34:28.840134Z","shell.execute_reply.started":"2025-01-29T02:34:28.827663Z","shell.execute_reply":"2025-01-29T02:34:28.839309Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"Get our data with features and labels. This has not been windowed yet. We'll pass in the window size so we can run models against different history lengths.","metadata":{}},{"cell_type":"code","source":"scaledTrainingDataByTicker, scaledTestDataByTicker = getScaledDataByTicker(tickerNames)","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:36:23.025319Z","iopub.execute_input":"2025-01-29T02:36:23.026109Z","iopub.status.idle":"2025-01-29T02:36:46.443690Z","shell.execute_reply.started":"2025-01-29T02:36:23.026083Z","shell.execute_reply":"2025-01-29T02:36:46.442837Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"scaledTrainingDataByTicker['aapl'].describe()","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:37:35.096877Z","iopub.execute_input":"2025-01-29T02:37:35.097262Z","iopub.status.idle":"2025-01-29T02:37:35.133658Z","shell.execute_reply.started":"2025-01-29T02:37:35.097235Z","shell.execute_reply":"2025-01-29T02:37:35.132872Z"},"trusted":true},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"         Dividends  Stock Splits         Gain       Gain05       Gain10  \\\ncount  2516.000000   2516.000000  2516.000000  2516.000000  2516.000000   \nmean      0.001676      0.002782     0.499558     0.323529     0.182830   \nstd       0.015637      0.139554     0.063767     0.467916     0.386604   \nmin       0.000000      0.000000     0.144785     0.000000     0.000000   \n25%       0.000000      0.000000     0.464423     0.000000     0.000000   \n50%       0.000000      0.000000     0.502185     0.000000     0.000000   \n75%       0.000000      0.000000     0.537182     1.000000     0.000000   \nmax       0.192500      7.000000     0.900019     1.000000     1.000000   \n\n            Gain15       Loss05       Loss10       Loss15  \ncount  2516.000000  2516.000000  2516.000000  2516.000000  \nmean      0.089030     0.303259     0.180843     0.113275  \nstd       0.284844     0.459758     0.384964     0.316992  \nmin       0.000000     0.000000     0.000000     0.000000  \n25%       0.000000     0.000000     0.000000     0.000000  \n50%       0.000000     0.000000     0.000000     0.000000  \n75%       0.000000     1.000000     0.000000     0.000000  \nmax       1.000000     1.000000     1.000000     1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dividends</th>\n      <th>Stock Splits</th>\n      <th>Gain</th>\n      <th>Gain05</th>\n      <th>Gain10</th>\n      <th>Gain15</th>\n      <th>Loss05</th>\n      <th>Loss10</th>\n      <th>Loss15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2516.000000</td>\n      <td>2516.000000</td>\n      <td>2516.000000</td>\n      <td>2516.000000</td>\n      <td>2516.000000</td>\n      <td>2516.000000</td>\n      <td>2516.000000</td>\n      <td>2516.000000</td>\n      <td>2516.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.001676</td>\n      <td>0.002782</td>\n      <td>0.499558</td>\n      <td>0.323529</td>\n      <td>0.182830</td>\n      <td>0.089030</td>\n      <td>0.303259</td>\n      <td>0.180843</td>\n      <td>0.113275</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.015637</td>\n      <td>0.139554</td>\n      <td>0.063767</td>\n      <td>0.467916</td>\n      <td>0.386604</td>\n      <td>0.284844</td>\n      <td>0.459758</td>\n      <td>0.384964</td>\n      <td>0.316992</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.144785</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.464423</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.502185</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.537182</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.192500</td>\n      <td>7.000000</td>\n      <td>0.900019</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"scaledTestDataByTicker['aapl'].describe()","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:37:39.919294Z","iopub.execute_input":"2025-01-29T02:37:39.920016Z","iopub.status.idle":"2025-01-29T02:37:39.947799Z","shell.execute_reply.started":"2025-01-29T02:37:39.919987Z","shell.execute_reply":"2025-01-29T02:37:39.947098Z"},"trusted":true},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"        Dividends  Stock Splits        Gain      Gain05      Gain10  \\\ncount  501.000000         501.0  501.000000  501.000000  501.000000   \nmean     0.003872           0.0    0.506051    0.333333    0.195609   \nstd      0.030440           0.0    0.055851    0.471876    0.397065   \nmin      0.000000           0.0    0.300885    0.000000    0.000000   \n25%      0.000000           0.0    0.472319    0.000000    0.000000   \n50%      0.000000           0.0    0.504688    0.000000    0.000000   \n75%      0.000000           0.0    0.540742    1.000000    0.000000   \nmax      0.250000           0.0    0.825851    1.000000    1.000000   \n\n           Gain15      Loss05      Loss10      Loss15  \ncount  501.000000  501.000000  501.000000  501.000000  \nmean     0.071856    0.265469    0.129741    0.063872  \nstd      0.258508    0.442024    0.336354    0.244770  \nmin      0.000000    0.000000    0.000000    0.000000  \n25%      0.000000    0.000000    0.000000    0.000000  \n50%      0.000000    0.000000    0.000000    0.000000  \n75%      0.000000    1.000000    0.000000    0.000000  \nmax      1.000000    1.000000    1.000000    1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dividends</th>\n      <th>Stock Splits</th>\n      <th>Gain</th>\n      <th>Gain05</th>\n      <th>Gain10</th>\n      <th>Gain15</th>\n      <th>Loss05</th>\n      <th>Loss10</th>\n      <th>Loss15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>501.000000</td>\n      <td>501.0</td>\n      <td>501.000000</td>\n      <td>501.000000</td>\n      <td>501.000000</td>\n      <td>501.000000</td>\n      <td>501.000000</td>\n      <td>501.000000</td>\n      <td>501.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.003872</td>\n      <td>0.0</td>\n      <td>0.506051</td>\n      <td>0.333333</td>\n      <td>0.195609</td>\n      <td>0.071856</td>\n      <td>0.265469</td>\n      <td>0.129741</td>\n      <td>0.063872</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.030440</td>\n      <td>0.0</td>\n      <td>0.055851</td>\n      <td>0.471876</td>\n      <td>0.397065</td>\n      <td>0.258508</td>\n      <td>0.442024</td>\n      <td>0.336354</td>\n      <td>0.244770</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.300885</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.472319</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.504688</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.540742</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.250000</td>\n      <td>0.0</td>\n      <td>0.825851</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"epochs = 300\ndaysOfHistory = 150\nsize = 250\nbatchSize = 32\ncolumnCount = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T02:37:42.672256Z","iopub.execute_input":"2025-01-29T02:37:42.672624Z","iopub.status.idle":"2025-01-29T02:37:42.676717Z","shell.execute_reply.started":"2025-01-29T02:37:42.672596Z","shell.execute_reply":"2025-01-29T02:37:42.675870Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"**Uncomment this cell to see my last models**","metadata":{}},{"cell_type":"code","source":"models = {}\n\n# modelAdam = basicLSTM(size, batchSize, daysOfHistory, columnCount)\n# modelAdam.compile(optimizer=Adam(), loss=MeanAbsoluteError(), metrics=['mean_absolute_error'])\n# models['Adam'] = modelAdam\n\n# modelAdamW = basicLSTM(size, batchSize, daysOfHistory, columnCount)\n# modelAdamW.compile(optimizer=AdamW(), loss=MeanAbsoluteError(), metrics=['mean_absolute_error'])\n# models['AdamW'] = modelAdamW\n\n# modelAdadelta = basicLSTM(size, batchSize, daysOfHistory, columnCount)\n# modelAdadelta.compile(optimizer=Adadelta(), loss=MeanAbsoluteError(), metrics=['mean_absolute_error'])\n# models['Adadelta'] = modelAdadelta\n\n# modelAdagrad = basicLSTM(size, batchSize, daysOfHistory, columnCount)\n# modelAdagrad.compile(optimizer=Adagrad(), loss=MeanAbsoluteError(), metrics=['mean_absolute_error'])\n# models['Adagrad'] = modelAdagrad\n\n# modelAdamax = basicLSTM(size, batchSize, daysOfHistory, columnCount)\n# modelAdamax.compile(optimizer=Adamax(), loss=MeanAbsoluteError(), metrics=['mean_absolute_error'])\n# models['Adamax'] = modelAdamax\n\n# modelAdafactor = basicLSTM(size, batchSize, daysOfHistory, columnCount)\n# modelAdafactor.compile(optimizer=Adafactor(), loss=MeanAbsoluteError(), metrics=['mean_absolute_error'])\n# models['Adafactor'] = modelAdafactor\n\n# modelFtrl = basicLSTM(size, batchSize, daysOfHistory, columnCount)\n# modelFtrl.compile(optimizer=Ftrl(), loss=MeanAbsoluteError(), metrics=['mean_absolute_error'])\n# models['Ftrl'] = modelFtrl\n\n# modelNadam = basicLSTM(size, batchSize, daysOfHistory, columnCount)\n# modelNadam.compile(optimizer=Nadam(), loss=MeanAbsoluteError(), metrics=['mean_absolute_error'])\n# models['Nadam'] = modelNadam\n\n# modelRMSprop = basicLSTM(size, batchSize, daysOfHistory, columnCount)\n# modelRMSprop.compile(optimizer=RMSprop(), loss=MeanAbsoluteError(), metrics=['mean_absolute_error'])\n# models['RMSprop'] = modelRMSprop\n\n# modelSGD = basicLSTM(size, batchSize, daysOfHistory, columnCount)\n# modelSGD.compile(optimizer=SGD(), loss=MeanAbsoluteError(), metrics=['mean_absolute_error'])\n# models['SGD'] = modelSGD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T02:37:46.116307Z","iopub.execute_input":"2025-01-29T02:37:46.116894Z","iopub.status.idle":"2025-01-29T02:37:46.121369Z","shell.execute_reply.started":"2025-01-29T02:37:46.116862Z","shell.execute_reply":"2025-01-29T02:37:46.120593Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"modelHistories = {}\nmodelsToRun = []\n\nfor modelName in models:\n    modelHistory = loadModelHistory(modelName)\n    if modelHistory is None:\n        modelsToRun.append(modelName)\n    else:\n        modelHistories[modelName] = modelHistory\n\nif len(modelsToRun) > 0:\n    print('Running', len(modelsToRun), 'models')\n    \n    print('Building training windows')\n    featureWindowsByTicker_train = getWindowsByTicker(scaledTrainingDataByTicker, daysOfHistory)\n    print('Building test windows')\n    featureWindowsByTicker_test = getWindowsByTicker(scaledTestDataByTicker, daysOfHistory)\n    print('Preparing training feature sets and labels')\n    X_train_regression, y_train_regression = getRegressionData(tickerNames, featureWindowsByTicker_train)\n    print('Preparing test feature sets and labels')\n    X_test_regression, y_test_regression = getRegressionData(tickerNames, featureWindowsByTicker_test)\n    \n    for modelName in modelsToRun:\n        model = models[modelName]\n        modelHistory = trainRegressionModel(modelName, model, X_train_regression, y_train_regression, X_test_regression, y_test_regression)\n        modelHistories[modelName] = modelHistory","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:37:49.016039Z","iopub.execute_input":"2025-01-29T02:37:49.016734Z","iopub.status.idle":"2025-01-29T02:37:49.022945Z","shell.execute_reply.started":"2025-01-29T02:37:49.016702Z","shell.execute_reply":"2025-01-29T02:37:49.021934Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def combineDict(dictionary, metric):\n    results = dictionary['Adam'][metric].copy()\n    for modelName in dictionary:\n        historyDf = dictionary[modelName]\n        results[modelName] = historyDf[metric]\n    results = results.drop([metric], axis=1)\n    return results\n\nif len(modelsToRun) > 0:\n    lossDf = combineDict(modelHistories, 'loss')\n    val_lossDf = combineDict(modelHistories, 'val_loss')\n    accuracyDf = combineDict(modelHistories, 'accuracy')\n    val_accuracyDf = combineDict(modelHistories, 'val_accuracy')\n    \n    lossDf.plot(title='Loss by optimizer')\n    val_lossDf.plot(title='Validation Loss by optimizer')\n    accuracyDf.plot(title='Accuracy by optimizer')\n    val_accuracyDf.plot(title='Validation Accuracy by optimizer')","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:37:52.019927Z","iopub.execute_input":"2025-01-29T02:37:52.020270Z","iopub.status.idle":"2025-01-29T02:37:52.026301Z","shell.execute_reply.started":"2025-01-29T02:37:52.020242Z","shell.execute_reply":"2025-01-29T02:37:52.025399Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# trainingFeatureWindowsByTicker = getWindowsByTicker(scaledTrainingDataByTicker, windowSize)\n# testFeatureWindowsByTicker = getWindowsByTicker(scaledTestDataByTicker, windowSize)","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:58.581767Z","iopub.status.idle":"2025-01-29T02:34:58.582053Z","shell.execute_reply.started":"2025-01-29T02:34:58.581917Z","shell.execute_reply":"2025-01-29T02:34:58.581930Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our regression training can use all of the data, so nothing special here. We'll use RandomSubsetDataGenerator to give us a random training set from the entire available dataset.","metadata":{}},{"cell_type":"code","source":"# X_train_regression, y_train_regression = getRegressionData(tickerNames, trainingFeatureWindowsByTicker)\n# X_test_regression, y_test_regression = getRegressionData(tickerNames, testFeatureWindowsByTicker)","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:58.583494Z","iopub.status.idle":"2025-01-29T02:34:58.583792Z","shell.execute_reply.started":"2025-01-29T02:34:58.583665Z","shell.execute_reply":"2025-01-29T02:34:58.583678Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our binary classifiers however need to have each set represented relatively evenly, so we'll do a random subset, but from each side, so we'll make a new RandomPairedSubsetDataGenerator to do that for us on binary models.","metadata":{}},{"cell_type":"code","source":"# # binary classifier with true being a gain above 0.5%\n# X_train_gain05_true, y_train_gain05_true, X_train_gain05_false, y_train_gain05_false = getBinaryData(tickerNames, trainingFeatureWindowsByTicker, 2)\n# X_test_gain05_true, y_test_gain05_true, X_test_gain05_false, y_test_gain05_false = getBinaryData(tickerNames, testFeatureWindowsByTicker, 2)\n\n# # binary classifier with true being a gain above 1.0%\n# X_train_gain10_true, y_train_gain10_true, X_train_gain10_false, y_train_gain10_false = getBinaryData(tickerNames, trainingFeatureWindowsByTicker, 3)\n# X_test_gain10_true, y_test_gain10_true, X_test_gain10_false, y_test_gain10_false = getBinaryData(tickerNames, testFeatureWindowsByTicker, 3)\n\n# # binary classifier with true being a gain above 1.5%\n# X_train_gain15_true, y_train_gain15_true, X_train_gain15_false, y_train_gain15_false = getBinaryData(tickerNames, trainingFeatureWindowsByTicker, 4)\n# X_test_gain15_true, y_test_gain15_true, X_test_gain15_false, y_test_gain15_false = getBinaryData(tickerNames, testFeatureWindowsByTicker, 4)\n\n# # binary classifier with true being a loss above 0.5%\n# X_train_loss05_true, y_train_loss05_true, X_train_loss05_false, y_train_loss05_false = getBinaryData(tickerNames, trainingFeatureWindowsByTicker, 5)\n# X_test_loss05_true, y_test_loss05_true, X_test_loss05_false, y_test_loss05_false = getBinaryData(tickerNames, testFeatureWindowsByTicker, 5)\n\n# # binary classifier with true being a loss above 1.0%\n# X_train_loss10_true, y_train_loss10_true, X_train_loss10_false, y_train_loss10_false = getBinaryData(tickerNames, trainingFeatureWindowsByTicker, 6)\n# X_test_loss10_true, y_test_loss10_true, X_test_loss10_false, y_test_loss10_false = getBinaryData(tickerNames, testFeatureWindowsByTicker, 6)\n\n# # binary classifier with true being a loss above 1.5%\n# X_train_loss15_true, y_train_loss15_true, X_train_loss15_false, y_train_loss15_false = getBinaryData(tickerNames, trainingFeatureWindowsByTicker, 7)\n# X_test_loss15_true, y_test_loss15_true, X_test_loss15_false, y_test_loss15_false = getBinaryData(tickerNames, testFeatureWindowsByTicker, 7)","metadata":{"execution":{"iopub.status.busy":"2025-01-29T02:34:58.584574Z","iopub.status.idle":"2025-01-29T02:34:58.584857Z","shell.execute_reply.started":"2025-01-29T02:34:58.584724Z","shell.execute_reply":"2025-01-29T02:34:58.584738Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Binary classifiers however need to have a balanced set of data, roughly half and half of each, otherwise you're skewed to favoring whichever is more common, so we'll return a randomized subset from each","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}